# Mock LLM configuration for query parameter resolution test
# Custom rule returns messages[0] (system message with resolved query parameters)
terminationGracePeriodSeconds: 3
ark:
  model:
    enabled: true
    name: mock-gpt-4.1
    type: completions
    provider: openai
    model: gpt-4.1
    pollInterval: 3s
    apiKey: mock-api-key

config:
  rules:
  - path: "/v1/models"
    method: "GET"
    response:
      status: 200
      content: |
        {
          "object": "list",
          "data": [
            {
              "id": "gpt-4",
              "object": "model",
              "created": 1687882411,
              "owned_by": "openai"
            },
            {
              "id": "gpt-4o-mini",
              "object": "model",
              "created": 1687882411,
              "owned_by": "openai"
            },
            {
              "id": "gpt-4.1",
              "object": "model",
              "created": 1687882411,
              "owned_by": "openai"
            },
            {
              "id": "gpt-5",
              "object": "model",
              "created": 1687882411,
              "owned_by": "openai"
            }
          ]
        }

  - path: "/v1/chat/completions"
    match: "@"
    response:
      status: 200
      content: |
        {
          "id": "mock-{{timestamp}}",
          "object": "chat.completion",
          "model": "{{jmes request body.model}}",
          "choices": [{
            "message": {{jmes request body.messages[0]}},
            "finish_reason": "stop"
          }]
        }
