---
title: End-to-End Testing
description: Comprehensive testing guide for ARK applications using Chainsaw and Hurl
---

# End-to-End Testing

Ark uses [Chainsaw](https://github.com/kyverno/chainsaw) to declaratively create resources, run scripts, and validate resources. For example, we can create agents, teams, and queries and validate the statuses of each and the success state of a query or evaluation.

## Setup

Install chainsaw for running tests locally:

```bash
# Install with Go
go install github.com/kyverno/chainsaw@latest

# Install with Homebrew
brew tap kyverno/chainsaw https://github.com/kyverno/chainsaw
brew install kyverno/chainsaw/chainsaw
```

## Running Tests Locally

To replicate the GitHub workflow environment locally:

```bash
# Optionally install k3d and create test cluster. If you are already running a
# cluster locally there is no need to run this step.
brew install k3d
k3d cluster create ark-e2e

# Set LLM config if you want to run tests that call models.
export E2E_TEST_AZURE_OPENAI_KEY=yourkey
export E2E_TEST_AZURE_OPENAI_BASE_URL=yoururl

# Run standard tests. Choose a specific path rather than '.' to run only a
# specific scenario. Use selectors to skip tests that require more complex
# dependencies:
#   !evaluated - skip tests that require the evaluator setup / images
#   !requires-images - skip tests that require additional images (filesystem mcp)
chainsaw test --config .chainsaw.yaml \
  --selector '!evaluated' \
  --selector '!requires-images' \
  ./
```

## Troubleshooting Tests

The following flags are extremely useful for troubleshooting test issues:

- `--pause-on-failure`: stop the test execution on the first failure - don't delete resources
- `--fail-fast`: abort immediately on first failure
- `--skip-delete`: don't delete the test resources or namespace at the end of the run (makes it easier to inspect resources)
- `--verbose`: show detaild logs

As an example, to run the full suite and pause as soon as there is any failure and allow the operator to inspect resources:

```bash
chainsaw test --config .chainsaw.yaml \
  --selector '!evaluated' \
  --selector '!requires-images' \
  --pause-on-failure \
  ./
```

When tests fail, inspect the created resources:

```bash
# Check the most recently created chainsaw test namespace (these naespaces
# always start with 'chainsaw-').
kubectl get namespace --sort-by=.metadata.creationTimestamp

# Check resources in test namespace, view logs, etc.
kubectl get all -n chainsaw-test-namespace
kubectl describe query -n chainsaw-test-namespace failed-query
```

## Developing New Tests

### Test Structure

Chainsaw tests follow this typical pattern:

```yaml
apiVersion: chainsaw.kyverno.io/v1alpha1
kind: Test
metadata:
  name: azure-openai-model-test
spec:
  steps:
    # Validate required environment variables
    - name: check-env-vars
      try:
        - script:
            content: |
              if [ -z "$E2E_TEST_AZURE_OPENAI_KEY" ]; then
                echo "E2E_TEST_AZURE_OPENAI_KEY is required"
                exit 1
              fi

    # Generate templated resources and apply them
    - name: apply
      try:
        - script:
            content: |
              kustomize build manifests | envsubst > /tmp/test-resources.yaml
        - apply:
            file: /tmp/test-resources.yaml
      finally:
        - script:
            content: rm -f /tmp/test-resources.yaml

    # Wait for model to reach ready state
    - name: assert
      try:
        - assert:
            file: assert-ready.yaml
```

### Writing Test Assertions

Create assertion files to validate resource states:

```yaml
# assert-ready.yaml
apiVersion: v1alpha1
kind: Model
metadata:
  name: test-model
status:
  conditions:
    - type: "Ready"
      status: "True"
      reason: "ModelResolved"
      message: "Model successfully resolved and validated"
      observedGeneration: 1
    - type: "Discovering"
      status: "False"
      reason: "ValidationComplete"
      message: "Model validation completed successfully"
      observedGeneration: 1
```

### Environment Variable Templating

Use `envsubst` for dynamic resource generation:

```yaml
# In your manifest template
apiVersion: v1alpha1
kind: Model
metadata:
  name: test-model
spec:
  source: azure-openai
  config:
    endpoint: $E2E_TEST_AZURE_OPENAI_BASE_URL
    apiKey: $E2E_TEST_AZURE_OPENAI_KEY
```

### Summarizing Chainsaw Test Results

For a quick summary of your Chainsaw test results, you can use the provided `scripts/chainsaw_summary.py` script. This script reads a Chainsaw JSON report and prints a concise table showing which tests passed or failed.

#### Usage

1. **Run your Chainsaw tests with JSON reporting enabled** (e.g., `chainsaw test ... --report-json /tmp/coverage-reports/chainsaw-report.json`).
2. **Run the summary script:**

   ```bash
   python3 scripts/chainsaw_summary.py /tmp/coverage-reports/chainsaw-report.json
   ```

   If you omit the report path, it defaults to `/tmp/coverage-reports/chainsaw-report.json`.

#### Example Output

```
Test Name                    | Result
------------------------------------------
query-model-target           | ✅ Passed
admission-failures           | ❌ Failed
query-label-selector         | ✅ Passed
query-event-recorder         | ✅ Passed
queries                      | ✅ Passed
models                       | ✅ Passed

```
3. **Include the evaluation summary with the `--append-evals` flag:**

   ```bash
    python3 scripts/chainsaw_summary.py --append-evals
   ```

#### Example Output

```
Evaluation                | Score     | Evaluator
--------------------------------------------------
chicago-weather-query     | 30       | evaluator-llm
research-query            | 95       | evaluator-llm
```

## Available Environment Variables for GitHub Actions

These environment variables are available on GitHub runners for your tests:

| Variable | Description |
|----------|-------------|
| `E2E_TEST_AZURE_OPENAI_KEY` | Azure OpenAI API key for testing model deployments |
| `E2E_TEST_AZURE_OPENAI_BASE_URL` | Azure OpenAI endpoint URL (e.g., `https://your-instance.openai.azure.com`) |

## HTTP API Testing with Hurl

### Overview

[Hurl](https://hurl.dev/) is used for testing HTTP APIs of services within chainsaw tests. It provides comprehensive HTTP client functionality with JSON path validation and test assertions.

### Service Test Structure

Services with HTTP APIs use this test structure:

```
services/{service-name}/test/
├── test.hurl              # HTTP test definitions
├── chainsaw-test.yaml     # Chainsaw integration
└── manifests/
    ├── pod-{service}-test.yaml   # Test pod with hurl image
    └── configmap.yaml            # ConfigMap mounting hurl files
```

### Basic Hurl Test Patterns

#### Health Check Testing
```hurl
# Test service health endpoint
GET http://service-name/health
HTTP 200
[Asserts]
body == "OK"
```

#### JSON API Testing
```hurl
# Test JSON endpoint with validation
GET http://service-name/api/endpoint
HTTP 200
[Asserts]
jsonpath "$.status" == "ready"
jsonpath "$.data" exists
jsonpath "$.data.items" count >= 1
```

#### POST with JSON Body
```hurl
# Send JSON data to API
PUT http://service-name/api/resource/session-id
Content-Type: application/json
{
  "data": {
    "field": "value",
    "items": ["item1", "item2"]
  }
}
HTTP 200
[Asserts]
jsonpath "$.success" == true
```

### Real-World Examples

#### ARK Broker Service

From `services/ark-broker/test/test.hurl`:

```hurl
# Check stream exists in global status
GET http://ark-broker/stream-statistics
HTTP 200
[Asserts]
jsonpath "$.queries.test-hurl-basic.total_chunks" == 4
jsonpath "$.queries.test-hurl-basic.completed" == false
jsonpath "$.queries.test-hurl-basic.chunk_types.content" == 3
jsonpath "$.queries.test-hurl-basic.chunk_types.finish_reason" == 1

# Complete the stream
POST http://ark-broker/stream/test-hurl-basic/complete
HTTP 200
[Asserts]
jsonpath "$.status" == "completed"
jsonpath "$.query" == "test-hurl-basic"
```

### Chainsaw Integration

#### Test Pod Setup
```yaml
# Pod with hurl Docker image
- apply:
    resource:
      apiVersion: v1
      kind: Pod
      metadata:
        name: service-test
      spec:
        containers:
        - name: test-client
          image: ghcr.io/orange-opensource/hurl:6.1.1
          command: ["sleep", "300"]
          volumeMounts:
          - name: test-files
            mountPath: /tests
        volumes:
        - name: test-files
          configMap:
            name: hurl-test-files
```

#### Test Execution
```yaml
# Execute hurl tests inside pod
- name: run-hurl-tests
  try:
  - script:
      content: |
        kubectl exec service-test -n $NAMESPACE -- hurl --test /tests/test.hurl
      timeout: 120s
```

### Combined Testing Pattern

Services typically combine HTTP API testing with ARK integration testing:

```yaml
# First test HTTP endpoints directly
- name: run-hurl-tests
  try:
  - script:
      content: kubectl exec test-pod -- hurl --test /tests/test.hurl

# Then test ARK integration
- name: test-ark-integration
  try:
  - assert:
      resource:
        apiVersion: ark.mckinsey.com/v1alpha1
        kind: Query
        status:
          phase: done
```

This validates both the service's HTTP API functionality and its integration with ARK resources.

## Testing Completions API Calls

If you need to validate or verify the values of parameters sent to an LLM endpoint, a simple echo server or similar can be set up as a model. This can be used as 'deterministic' LLM that gives predicatable output.

As an example, this test at:

```
https://github.com/mckinsey/agents-at-scale-ark/tree/main/tests/query-parameter-ref
```

Is used to check whether the parameters provided via a query to an agent are expanded correctly. Rather than asking the LLM to give back a response in a prompt, which is brittle, a mock LLM server is used which echos back the response - this can then be validated deterministically by checking the `query.Responses` field. The mock server is defined in `a00-mock-server.yaml`.

## Next Steps

- [Services](/developer-guide/services) - Learn about ARK services
- [Observability](/developer-guide/observability) - Monitor your ARK applications
